{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query via RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, BingSearchRun\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper, BingSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the LLM and neo4j graph\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "llm_json = ChatOpenAI(model=\"gpt-4o\", temperature=0, model_kwargs={\"response_format\": {\"type\": \"json_object\"}})\n",
    "\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 70, offset: 69} for query: \"UNWIND $data AS row MATCH (n:`Document`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, object, location, or event entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting person, object, location, or event entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('keyword', $query, {limit: 2})\n",
    "            YIELD node, score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def retriever(question: str):\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "        {structured_data}\n",
    "        Unstructured data:\n",
    "        {\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be as elaborate as possible.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def invoke_chain(question: str, chat_history):\n",
    "    print(\"invoke chain called\")\n",
    "    graph.query(\n",
    "        \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "    print(\"fulltext index created\")\n",
    "    if chat_history:\n",
    "        return chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chat_history\": chat_history\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        return chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "########################################################### Web Search Tool ###########################################################\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(max_results=25)\n",
    "web_search_tool = DuckDuckGoSearchRun(api_wrapper=wrapper)\n",
    "\n",
    "########################################################### Query Transformation ###########################################################\n",
    "query_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"\n",
    "            You are an expert at crafting web search queries for research questions.\n",
    "            More often than not, a user will ask a basic question that they wish to learn more about, however it might not be in the best format. \n",
    "            Reword their query to be the most effective web search string possible.\n",
    "            Return the JSON with a single key 'query' with no premable or explanation. \n",
    "            \n",
    "            Question to transform: {question} \n",
    "         \"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "query_chain = query_prompt | llm | JsonOutputParser()\n",
    "\n",
    "############################################################# Graph State #############################################################\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        search_query: revised question for web search\n",
    "        context: web_search result\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    search_query : str\n",
    "    context : str\n",
    "\n",
    "# Node - Generate\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Generating Final Response\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Answer Generation\n",
    "    generation = invoke_chain(question, None)\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node - Query Transformation\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform user question to web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended search query\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Step: Optimizing Query for Web Search\")\n",
    "    question = state['question']\n",
    "    gen_query = query_chain.invoke({\"question\": question})\n",
    "    search_query = gen_query[\"query\"]\n",
    "    return {\"search_query\": search_query}\n",
    "\n",
    "\n",
    "# Node - Web Search\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to context\n",
    "    \"\"\"\n",
    "\n",
    "    search_query = state['search_query']\n",
    "    print(f'Step: Searching the Web for: \"{search_query}\"')\n",
    "    \n",
    "    # Web search tool call\n",
    "    search_result = web_search_tool.invoke(search_query)\n",
    "    return {\"context\": search_result}\n",
    "    \n",
    "    # let the function do nothing and return the same state\n",
    "    # return state\n",
    "    \n",
    "\n",
    "\n",
    "# Conditional Edge, Routing\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    route question to web search or generation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step: Routing Query\")\n",
    "    question = state['question']\n",
    "    structured_data = structured_retriever(question)\n",
    "    \n",
    "    if len(structured_data) != 0:\n",
    "        print(\"Step: Context Found, Routing to Generation\")\n",
    "        return \"generate\"\n",
    "    elif len(structured_data) == 0:\n",
    "        print(\"Step: Context Not Found, Routing to Web Search\")\n",
    "        return \"websearch\"\n",
    "    \n",
    "def build_workflow():\n",
    "    \"\"\"\n",
    "    Build the workflow for the graph\n",
    "    \"\"\"\n",
    "    # Build the nodes\n",
    "    workflow = StateGraph(GraphState)\n",
    "    workflow.add_node(\"websearch\", web_search)\n",
    "    workflow.add_node(\"transform_query\", transform_query)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "\n",
    "    # Build the edges\n",
    "    workflow.set_conditional_entry_point(\n",
    "        route_question,\n",
    "        {\n",
    "            \"websearch\": \"transform_query\",\n",
    "            \"generate\": \"generate\",\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"transform_query\", \"websearch\")\n",
    "    workflow.add_edge(\"websearch\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "\n",
    "    # Compile the workflow\n",
    "    local_agent = workflow.compile()\n",
    "\n",
    "    return local_agent\n",
    "\n",
    "def run_agent(query, local_agent):\n",
    "    output = local_agent.invoke({\"question\": query})\n",
    "    print(\"=======\")\n",
    "    display(Markdown(output[\"generation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Routing Query\n",
      "Step: Context Not Found, Routing to Web Search\n",
      "Step: Optimizing Query for Web Search\n",
      "Step: Searching the Web for: \"recent government initiatives in public transportation with examples and cities\"\n",
      "Step: Generating Final Response\n",
      "invoke chain called\n",
      "fulltext index created\n",
      "=======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Several cities have introduced novel initiatives to enhance public transportation, focusing on sustainability, efficiency, and citizen well-being. Here are a few examples:\n",
       "\n",
       "1. **Solaris - \"Eco-Rail\" Metro System and \"Cycle Share\" Bike-Sharing Program**:\n",
       "   - **Eco-Rail Metro System**: Solaris has developed an extensive metro system that prioritizes eco-friendly mobility. This initiative has encouraged more active lifestyles among residents, leading to a 25% increase in the number of people commuting by public transit or cycling.\n",
       "   - **Cycle Share Bike-Sharing Program**: Complementing the metro system, Solaris introduced a bike-sharing program that has further promoted sustainable transportation. This program has contributed to a significant reduction in air pollution and improved public health, with a 15% drop in respiratory-related illnesses.\n",
       "\n",
       "2. **Elysia - Solar-Powered Streetlights**:\n",
       "   - **Solar-Powered Streetlights**: Elysia has replaced conventional streetlights with solar-powered alternatives. These streetlights collect sunlight during the day and store energy in batteries to illuminate the city at night. This initiative not only reduces energy consumption but also engages local schools in the design and installation process, fostering community involvement and education.\n",
       "\n",
       "3. **Utopolis - Solar-Powered Streetlights and Rainwater Harvesting**:\n",
       "   - **Solar-Powered Streetlights**: Similar to Elysia, Utopolis has implemented solar-powered streetlights throughout the city. These streetlights are strategically placed to ensure adequate illumination in streets, parks, and public areas, promoting sustainable lighting solutions.\n",
       "   - **Rainwater Harvesting**: Utopolis has also introduced rainwater harvesting systems in public buildings. This initiative reduces water usage by collecting and storing rainwater for non-potable purposes such as irrigation and cleaning, demonstrating a commitment to sustainable water management.\n",
       "\n",
       "These initiatives reflect a broader trend among cities to innovate and invest in sustainable public transportation and infrastructure, ultimately enhancing the quality of life for their residents."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test it out!\n",
    "local_agent = build_workflow()\n",
    "test_query = \"What are some examples of novel initiatives kickstarted by governments regarding public transportation? Please list a few, together with their respective cities.\"\n",
    "run_agent(test_query, local_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
